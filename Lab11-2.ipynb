{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.139091453\n",
      "Epoch: 0002 cost = 0.054334230\n",
      "Epoch: 0003 cost = 0.039152828\n",
      "Epoch: 0004 cost = 0.033912304\n",
      "Epoch: 0005 cost = 0.027577225\n",
      "Epoch: 0006 cost = 0.027127721\n",
      "Epoch: 0007 cost = 0.023791519\n",
      "Epoch: 0008 cost = 0.023780140\n",
      "Epoch: 0009 cost = 0.024759805\n",
      "Epoch: 0010 cost = 0.020484630\n",
      "Epoch: 0011 cost = 0.017005853\n",
      "Epoch: 0012 cost = 0.020540934\n",
      "Epoch: 0013 cost = 0.023551873\n",
      "Epoch: 0014 cost = 0.014639197\n",
      "Epoch: 0015 cost = 0.029460054\n",
      "Epoch: 0016 cost = 0.016297161\n",
      "Epoch: 0017 cost = 0.015860078\n",
      "Epoch: 0018 cost = 0.017379001\n",
      "Epoch: 0019 cost = 0.023954951\n",
      "Epoch: 0020 cost = 0.018790967\n",
      "Epoch: 0021 cost = 0.015032721\n",
      "Epoch: 0022 cost = 0.019075310\n",
      "Epoch: 0023 cost = 0.016969801\n",
      "Epoch: 0024 cost = 0.018007222\n",
      "Epoch: 0025 cost = 0.015717998\n",
      "Epoch: 0026 cost = 0.017858515\n",
      "Epoch: 0027 cost = 0.018112548\n",
      "Epoch: 0028 cost = 0.017693828\n",
      "Epoch: 0029 cost = 0.017721742\n",
      "Epoch: 0030 cost = 0.033677880\n",
      "Epoch: 0031 cost = 0.024831109\n",
      "Epoch: 0032 cost = 0.018849633\n",
      "Epoch: 0033 cost = 0.015289657\n",
      "Epoch: 0034 cost = 0.023159942\n",
      "Epoch: 0035 cost = 0.016886580\n",
      "Epoch: 0036 cost = 0.011449599\n",
      "Epoch: 0037 cost = 0.016631092\n",
      "Epoch: 0038 cost = 0.030066047\n",
      "Epoch: 0039 cost = 0.018219613\n",
      "Epoch: 0040 cost = 0.016283117\n",
      "Epoch: 0041 cost = 0.024519620\n",
      "Epoch: 0042 cost = 0.013276034\n",
      "Epoch: 0043 cost = 0.028627890\n",
      "Epoch: 0044 cost = 0.040789048\n",
      "Epoch: 0045 cost = 0.020561715\n",
      "Epoch: 0046 cost = 0.021167770\n",
      "Epoch: 0047 cost = 0.027154914\n",
      "Epoch: 0048 cost = 0.020698005\n",
      "Epoch: 0049 cost = 0.022340262\n",
      "Epoch: 0050 cost = 0.021009096\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 50\n",
    "batch_size = 100\n",
    "num = mnist.train.num_examples//batch_size\n",
    "\n",
    "# input placeholder\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "x_img = tf.reshape(x, [-1,28,28,1])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# feature extraction\n",
    "w1 = tf.Variable(tf.random_normal([3,3,1,32], stddev = 0.01))\n",
    "                                      # stddev -> 표준편차 0.01의 정규분포\n",
    "l1 = tf.nn.conv2d(x_img, w1, strides = [1,1,1,1], padding=\"SAME\")\n",
    "l1 = tf.nn.relu(l1)\n",
    "l1 = tf.nn.max_pool(l1, ksize = [1,2,2,1], strides = [1,2,2,1],\n",
    "                   padding = \"SAME\") # [?, 14,14,32]\n",
    "\"[?,28,28,1] -> [?, 28, 28, 32] -> [?,14,14,32]\"\n",
    "                            # feature을 덜 세세하게 함으로써 overfitting 을 방지\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal([3,3,32,64], stddev = 0.01)) # [?, 14,14, 32]\n",
    "l2 = tf.nn.conv2d(l1, w2, strides = [1,1,1,1], padding = \"SAME\") \n",
    "l2 = tf.nn.relu(l2)\n",
    "l2 = tf.nn.max_pool(l2, ksize = [1,2,2,1], strides = [1,2,2,1], # [?,7,7,32]\n",
    "                   padding=\"SAME\")\n",
    "l2 = tf.reshape(l2, [-1, 7*7*64]) # FC 를 위해 쫙 펼침\n",
    "\"[?,14,14,32] -> [?,14,14,64] -> [?,14,14,64] -> [?,14*14*64]\"\n",
    "\n",
    "w3 = tf.get_variable(\"w3\", shape=[7*7*64,10],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b = tf.get_variable(\"b3\", shape = [10],\n",
    "                   initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "# classification\n",
    "hypothesis = tf.matmul(l2,w3)+b\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits\n",
    "                      (logits = hypothesis, labels = y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "writer = tf.summary.FileWriter(\"./MNIST_graph\")\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for step in range(num):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {x:batch_xs, y:batch_ys}\n",
    "        c, _= sess.run([cost,optimizer], feed_dict = feed_dict)\n",
    "        avg_cost += c/num\n",
    "    print(\"Epoch:\", \"%04d\"%(epoch+1), \"cost =\", \"{:011.9f}\".format(avg_cost))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  98.75000%\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype = tf.float32))\n",
    "a = sess.run(accuracy, feed_dict={x:mnist.test.images,\n",
    "                                                y:mnist.test.labels})\n",
    "print(\"accuracy : {:10.5%}\".format(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.744329727\n",
      "Epoch: 0002 cost = 0.348777859\n",
      "Epoch: 0003 cost = 0.328508723\n",
      "Epoch: 0004 cost = 0.293287922\n",
      "Epoch: 0005 cost = 0.281662331\n",
      "Epoch: 0006 cost = 0.268073255\n",
      "Epoch: 0007 cost = 0.270948216\n",
      "Epoch: 0008 cost = 0.259456737\n",
      "Epoch: 0009 cost = 0.274750360\n",
      "Epoch: 0010 cost = 0.274909263\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# l1 image shape=(?,28,28,1)\n",
    "#parameters\n",
    "training_epochs = 10\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "num = mnist.train.num_examples//batch_size\n",
    "\n",
    "# Placeholers\n",
    "x = tf.placeholder(tf.float32, [None,784])\n",
    "x_img = tf.reshape(x, [-1, 28,28,1])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([3,3,1, 32], stddev = 0.01))\n",
    "b1 = tf.Variable(tf.random_normal([32], stddev = 0.01)) # 32개의 filter\n",
    "l1 = tf.nn.conv2d(x_img, w1, strides = [1,1,1,1], padding = \"SAME\")+b1\n",
    "l1 = tf.nn.relu(l1)\n",
    "l1 = tf.nn.max_pool(l1, ksize = [1,2,2,1], strides = [1,2,2,1],\n",
    "                   padding = \"SAME\") # 원래는 이게 input\n",
    "l1 = tf.nn.dropout(l1, keep_prob = keep_prob)\n",
    "\n",
    "# l2 image shape=(?,14,14,32)\n",
    "w2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "b2 = tf.Variable(tf.random_normal([64]))\n",
    "l2 = tf.nn.conv2d(l1, w2, strides = [1,1,1,1], padding = \"SAME\")+b2\n",
    "l2 = tf.nn.relu(l2)\n",
    "l2 = tf.nn.max_pool(l2, ksize = [1,2,2,1], strides=[1,2,2,1],\n",
    "                   padding = \"SAME\")\n",
    "l2 = tf.nn.dropout(l2, keep_prob = keep_prob)\n",
    "\n",
    "# l3 image shape=(?,7,7,64)\n",
    "w3 = tf.Variable(tf.random_normal([3,3,64,128], stddev = 0.01))\n",
    "b3 = tf.Variable(tf.random_normal([128], stddev = 0.01))\n",
    "l3 = tf.nn.conv2d(l2, w3, strides = [1,1,1,1], padding=\"SAME\")+b3\n",
    "l3 = tf.nn.relu(l3)\n",
    "l3 = tf.nn.max_pool(l3, ksize = [1,2,2,1], strides = [1,2,2,1],\n",
    "                   padding=\"SAME\")\n",
    "l3 = tf.nn.dropout(l3, keep_prob= keep_prob)\n",
    "l3= tf.reshape(l3, [-1,4*4*128])\n",
    "\n",
    "# classification\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape = [4*4*128,625],\n",
    "                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "B1 = tf.get_variable(\"B11\", shape = [625],\n",
    "                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "L1 = tf.matmul(l3,W1)+B1\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob = keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape = [625,10],\n",
    "                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "B2 = tf.get_variable(\"B2\", shape = [10],\n",
    "                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "hypothesis = tf.matmul(L1, W2) + B2\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis,\n",
    "                                labels = y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for step in range(num):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        c, _= sess.run([cost,optimizer], feed_dict = {x:batch_xs, y:batch_ys,\n",
    "                      keep_prob:0.7})\n",
    "        avg_cost += c/num\n",
    "    print(\"Epoch:\", \"%04d\"%(epoch+1), \"cost =\", \"{:.9f}\".format(avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype = tf.float32))\n",
    "a = sess.run(accuracy, feed_dict={x:mnist.test.images,\n",
    "                                                y:mnist.test.labels,\n",
    "                                 keep_prob:1})\n",
    "print(\"accuracy : {:10.5%}\".format(a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
