{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-fd6f3ae84f21>:10: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "\n",
    "tf.set_random_seed(333) # for reproducibility\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True) # one_hot 으로 y 데이터를 읽어옴\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits(x,y,z):\n",
    "    return tf.matmul(x,y)+z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem of GradientDescent &Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam optimizer  \n",
    "- GradientDescent 의 일종이지만 gradient 를 매개변수의 이동평균을 사용하여 계산\n",
    "- step size 가 더 커지면서 적은 step size 로도 가능!\n",
    "- 계산량 증가 및 모델 크기가 3배가 됨(평균, 분산 추가) 하지만 적은 step size로도 좋은 정확도를 만들어 낼 수 있음\n",
    "- 즉 gradient descnet의 에러보다 훨씬 빠르게 줄어듦\n",
    "- ex) Adam step : 400~800 but, Gradient Descent : 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RELU\n",
    "- 앞으로 갈수록 w 의 값이 결과에 거의 영향을 안미치는 문제 해결\n",
    "- 0에서 1에 국한되지 않고 0~inf로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_cost : 14.18374\n",
      "average_cost : 10.14060\n",
      "average_cost : 9.86606\n",
      "average_cost : 0.44760\n",
      "average_cost : 5.00859\n",
      "average_cost : 4.41425\n",
      "average_cost : 0.82572\n",
      "average_cost : 3.93973\n",
      "average_cost : 0.69115\n",
      "average_cost : 0.22258\n",
      "average_cost : 2.09496\n",
      "average_cost : 2.06805\n",
      "average_cost : 2.91023\n",
      "average_cost : 4.35140\n",
      "average_cost : 2.56475\n",
      "average_cost : 1.58754\n",
      "average_cost : 0.05158\n",
      "average_cost : 0.09049\n",
      "average_cost : 2.03332\n",
      "average_cost : 0.33566\n",
      "average_cost : 0.87890\n",
      "average_cost : 0.81062\n",
      "average_cost : 0.55758\n",
      "average_cost : 0.70882\n",
      "average_cost : 0.00032\n",
      "average_cost : 0.59343\n",
      "average_cost : 0.19194\n",
      "average_cost : 0.04823\n",
      "average_cost : 0.04123\n",
      "average_cost : 0.45648\n",
      "average_cost : 0.03428\n",
      "average_cost : 0.11447\n",
      "average_cost : 0.47420\n",
      "average_cost : 0.01584\n",
      "average_cost : 0.02121\n",
      "average_cost : 0.00910\n",
      "average_cost : 0.20934\n",
      "average_cost : 0.04502\n",
      "average_cost : 0.61712\n",
      "average_cost : 0.07573\n",
      "average_cost : 0.07121\n",
      "average_cost : 0.05173\n",
      "average_cost : 0.00720\n",
      "average_cost : 0.05592\n",
      "average_cost : 0.04154\n",
      "average_cost : 0.55221\n",
      "average_cost : 0.14094\n",
      "average_cost : 0.03549\n",
      "average_cost : 0.15704\n",
      "average_cost : 0.09686\n",
      "accuacy : 96.55000%\n"
     ]
    }
   ],
   "source": [
    "# tensorboard reset\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#paramiters\n",
    "learning_rate=0.01\n",
    "batch_size = 100\n",
    "training_epochs = 50\n",
    "num = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "# input place holders\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "with tf.name_scope(\"layer1\"):\n",
    "    w1 = tf.Variable(tf.random_normal([784,256]))\n",
    "    b1 = tf.Variable(tf.random_normal([256]))\n",
    "    layer1 = tf.nn.relu(logits(x,w1,b1))\n",
    "\n",
    "with tf.name_scope(\"layer2\"):\n",
    "    w2 = tf.Variable(tf.random_normal([256,256]))\n",
    "    b2 = tf.Variable(tf.random_normal([256]))\n",
    "    layer2 = tf.nn.relu(logits(layer1, w2, b2))\n",
    "\n",
    "with tf.name_scope(\"hypothesis\"):\n",
    "    w3 = tf.Variable(tf.random_normal([256,10]))\n",
    "    b3 = tf.Variable(tf.random_normal([10]))\n",
    "    hypothesis = logits(layer2, w3,b3)\n",
    "\n",
    "#define cost/Loss $ optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis,\n",
    "                                    labels = y))\n",
    "tf.summary.scalar(\"cost\",cost)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# tensorboard node\n",
    "summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"./MNIST_graph\")\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# add session graph\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost=0\n",
    "    for step in range(num):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        s,c,_ = sess.run([summary, cost, optimizer], \n",
    "                         feed_dict={x:batch_x, y:batch_y})\n",
    "        writer.add_summary(s, global_step=step+epoch*mnist.train.num_examples)\n",
    "        avg_cost+=c/num\n",
    "    print(\"average_cost : {:.5f}\".format(c))\n",
    "\n",
    "# accuacy\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(\n",
    "    tf.argmax(hypothesis, 1), tf.argmax(y,1)), dtype=tf.float32))\n",
    "\n",
    "# tf.argmax(hypothesis, 1) 은 int 형식 but, y는 float 형식이기 때문에\n",
    "# y 에도 tf.argmax(y,1) 을 통해 형식을 맞춰줌!!!\n",
    "\n",
    "a= sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels})\n",
    "print(\"accuacy : {:.5%}\".format(a))\n",
    "# => training 시켜놓은 모델을 통해 예측하기만 하면 되기 때문에\n",
    "# optimizer node를 실행시킬 이유가 전혀 없음!!!!!\n",
    "\n",
    "# anaconda 에서 tensorboard 실행 : tensorboard --logdir=./directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem of Initializaition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xavier initialization tensorflow\n",
    "\n",
    "- 몇개의 입력값과 출력값을 갖는지에 따라 초깃값을 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_cost : 0.18663\n",
      "average_cost : 0.09842\n",
      "average_cost : 0.06180\n",
      "average_cost : 0.13780\n",
      "average_cost : 0.04258\n",
      "average_cost : 0.23405\n",
      "average_cost : 0.00801\n",
      "average_cost : 0.03018\n",
      "average_cost : 0.00273\n",
      "average_cost : 0.06974\n",
      "average_cost : 0.07376\n",
      "average_cost : 0.04162\n",
      "average_cost : 0.04981\n",
      "average_cost : 0.02201\n",
      "average_cost : 0.07055\n",
      "average_cost : 0.06270\n",
      "average_cost : 0.04189\n",
      "average_cost : 0.02749\n",
      "average_cost : 0.13337\n",
      "average_cost : 0.02748\n",
      "average_cost : 0.02261\n",
      "average_cost : 0.05832\n",
      "average_cost : 0.01765\n",
      "average_cost : 0.07701\n",
      "average_cost : 0.03051\n",
      "average_cost : 0.05037\n",
      "average_cost : 0.00451\n",
      "average_cost : 0.00376\n",
      "average_cost : 0.00053\n",
      "average_cost : 0.00875\n",
      "average_cost : 0.05680\n",
      "average_cost : 0.04006\n",
      "average_cost : 0.01512\n",
      "average_cost : 0.01581\n",
      "average_cost : 0.06060\n",
      "average_cost : 0.03154\n",
      "average_cost : 0.04790\n",
      "average_cost : 0.01118\n",
      "average_cost : 0.00239\n",
      "average_cost : 0.02470\n",
      "average_cost : 0.00972\n",
      "average_cost : 0.01610\n",
      "average_cost : 0.01936\n",
      "average_cost : 0.00063\n",
      "average_cost : 0.03553\n",
      "average_cost : 0.00001\n",
      "average_cost : 0.00382\n",
      "average_cost : 0.02028\n",
      "average_cost : 0.00000\n",
      "average_cost : 0.00018\n",
      "accuacy : 97.48000%\n"
     ]
    }
   ],
   "source": [
    "# tensorboard reset\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#paramiters\n",
    "learning_rate=0.01\n",
    "batch_size = 100\n",
    "training_epochs = 50\n",
    "num = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "with tf.name_scope(\"layer1\"):\n",
    "    w1 = tf.get_variable(\"w1\", shape = [784,256],\n",
    "                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.get_variable(\"b1\", shape = [256],\n",
    "                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "    l1 = tf.nn.relu(logits(x,w1,b1))\n",
    "    \n",
    "with tf.name_scope(\"layer2\"):\n",
    "    w2 = tf.get_variable(\"w2\", shape = [256,256],\n",
    "                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.get_variable(\"b2\", shape = [256],\n",
    "                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "    l2 = tf.nn.relu(logits(l1,w2,b2))\n",
    "    \n",
    "with tf.name_scope(\"layer3\"):\n",
    "    w3 = tf.get_variable(\"w3\", shape = [256,10],\n",
    "                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.get_variable(\"b3\", shape = [10],\n",
    "                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "    hypothesis = logits(l2,w3,b3)\n",
    "    \n",
    "#define cost/Loss $ optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis,\n",
    "                                    labels = y))\n",
    "tf.summary.scalar(\"cost\",cost)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# tensorboard node\n",
    "summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"./MNIST_graph\")\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# add session graph\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost=0\n",
    "    for step in range(num):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        s,c,_ = sess.run([summary, cost, optimizer], \n",
    "                         feed_dict={x:batch_x, y:batch_y})\n",
    "        writer.add_summary(s, global_step=step+epoch*mnist.train.num_examples)\n",
    "        avg_cost+=c/num\n",
    "    print(\"average_cost : {:.5f}\".format(c))\n",
    "\n",
    "# accuacy\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(\n",
    "    tf.argmax(hypothesis, 1), tf.argmax(y,1)), dtype=tf.float32))\n",
    "\n",
    "# tf.argmax(hypothesis, 1) 은 int 형식 but, y는 float 형식이기 때문에\n",
    "# y 에도 tf.argmax(y,1) 을 통해 형식을 맞춰줌!!!\n",
    "\n",
    "a= sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels})\n",
    "print(\"accuacy : {:.5%}\".format(a))\n",
    "# => training 시켜놓은 모델을 통해 예측하기만 하면 되기 때문에\n",
    "# optimizer node를 실행시킬 이유가 전혀 없음!!!!!\n",
    "\n",
    "# anaconda 에서 tensorboard 실행 : tensorboard --logdir=./directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Problem of Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "- 랜덤으로 몇개의 layer을 버린 뒤 실행\n",
    "- 실행할 때마다 랜덤값은 달라짐\n",
    "- 몇개의 입력값을 버리는 것과 같기 때문에 test set 에서는 keep_prod 를 1로 설정해주어야 함!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_cost : 0.26704\n",
      "average_cost : 0.17772\n",
      "average_cost : 0.37853\n",
      "average_cost : 0.20183\n",
      "average_cost : 0.19908\n",
      "average_cost : 0.11749\n",
      "average_cost : 0.12567\n",
      "average_cost : 0.28998\n",
      "average_cost : 0.31826\n",
      "average_cost : 0.20977\n",
      "average_cost : 0.22589\n",
      "average_cost : 0.22677\n",
      "average_cost : 0.21671\n",
      "average_cost : 0.14707\n",
      "average_cost : 0.13988\n",
      "average_cost : 0.19199\n",
      "average_cost : 0.16821\n",
      "average_cost : 0.18863\n",
      "average_cost : 0.32551\n",
      "average_cost : 0.23350\n",
      "average_cost : 0.24057\n",
      "average_cost : 0.50760\n",
      "average_cost : 0.40387\n",
      "average_cost : 0.38214\n",
      "average_cost : 0.28324\n",
      "average_cost : 0.06814\n",
      "average_cost : 0.17852\n",
      "average_cost : 0.25214\n",
      "average_cost : 0.16326\n",
      "average_cost : 0.23111\n",
      "average_cost : 0.28513\n",
      "average_cost : 0.17517\n",
      "average_cost : 0.14888\n",
      "average_cost : 0.12256\n",
      "average_cost : 0.29147\n",
      "average_cost : 0.09814\n",
      "average_cost : 0.17129\n",
      "average_cost : 0.18240\n",
      "average_cost : 0.04158\n",
      "average_cost : 0.17980\n",
      "average_cost : 0.19433\n",
      "average_cost : 0.09985\n",
      "average_cost : 0.04079\n",
      "average_cost : 0.11485\n",
      "average_cost : 0.45314\n",
      "average_cost : 0.11343\n",
      "average_cost : 0.02595\n",
      "average_cost : 0.14279\n",
      "average_cost : 0.11374\n",
      "average_cost : 0.06246\n",
      "accuacy : 96.76000%\n"
     ]
    }
   ],
   "source": [
    "# tensorboard reset\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#paramiters\n",
    "learning_rate=0.01\n",
    "batch_size = 100\n",
    "training_epochs = 50\n",
    "num = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "# dropout (keep_prob) rate 0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "with tf.name_scope(\"layer1\"):\n",
    "    w1 = tf.get_variable(\"w1\", shape = [784,512],\n",
    "                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.get_variable(\"b1\", shape = [512],\n",
    "                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "    l1 = tf.nn.relu(logits(x,w1,b1))\n",
    "    l1 = tf.nn.dropout(l1, keep_prob = keep_prob)\n",
    "    \n",
    "with tf.name_scope(\"layer2\"):\n",
    "    w2 = tf.get_variable(\"w2\", shape = [512,512],\n",
    "                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.get_variable(\"b2\", shape = [512],\n",
    "                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "    l2 = tf.nn.relu(logits(l1,w2,b2))\n",
    "    l2 = tf.nn.dropout(l2, keep_prob = keep_prob)\n",
    "    \n",
    "with tf.name_scope(\"layer3\"):\n",
    "    w3 = tf.get_variable(\"w3\", shape = [512,10],\n",
    "                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.get_variable(\"b3\", shape = [10],\n",
    "                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "    hypothesis = logits(l2,w3,b3)\n",
    "    \n",
    "#define cost/Loss $ optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis,\n",
    "                                    labels = y))\n",
    "tf.summary.scalar(\"cost\",cost)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# tensorboard node\n",
    "summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"./MNIST_graph\")\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# add session graph\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost=0\n",
    "    for step in range(num):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        s,c,_ = sess.run([summary, cost, optimizer], \n",
    "                         feed_dict={x:batch_x, y:batch_y, keep_prob : 0.7})\n",
    "        writer.add_summary(s, global_step=step+epoch*mnist.train.num_examples)\n",
    "        avg_cost+=c/num\n",
    "    print(\"average_cost : {:.5f}\".format(c))\n",
    "\n",
    "# accuacy\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(\n",
    "    tf.argmax(hypothesis, 1), tf.argmax(y,1)), dtype=tf.float32))\n",
    "\n",
    "# tf.argmax(hypothesis, 1) 은 int 형식 but, y는 float 형식이기 때문에\n",
    "# y 에도 tf.argmax(y,1) 을 통해 형식을 맞춰줌!!!\n",
    "\n",
    "a= sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels,\n",
    "                                 keep_prob:1})\n",
    "print(\"accuacy : {:.5%}\".format(a))\n",
    "# => training 시켜놓은 모델을 통해 예측하기만 하면 되기 때문에\n",
    "# optimizer node를 실행시킬 이유가 전혀 없음!!!!!\n",
    "\n",
    "# anaconda 에서 tensorboard 실행 : tensorboard --logdir=./directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "- tf.train.AdadeltaOptimizer\n",
    "- tf.train.AdagradOptimizer\n",
    "- tf.train.AdagradDAOptimizer\n",
    "- tf.train.MomentumOptimizer\n",
    "- tf.train.AdamOptimizer           => 적은 step 으로도 cost 를 많이 줄일 수 있음\n",
    "- tf.train.FtrlOptimizer\n",
    "- tf.train.ProximalGradientDescentOptimizer\n",
    "- tf.train.ProximalAdagradOptimizer\n",
    "- tf.train.RMSPropOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "- 학습 속도 개선(큰 learning rate으로 적은 step size 가능)\n",
    "- 가중치 초깃값의 의존도 줄어듦(평균0, 분산1의 output)\n",
    "- overfitting 의 문제해결(dropout / regulization 대체 가능)\n",
    "- Gradient Vanishing 문제 해결\n",
    "\n",
    "<img src = https://camo.githubusercontent.com/71f966091fd4696bc534ec9963806b65e962a7de/68747470733a2f2f6b7261747a6572742e6769746875622e696f2f696d616765732f626e5f6261636b706173732f626e5f616c676f726974686d2e504e47 width = 300><br>\n",
    "\n",
    "- train set => 각각의 input 이 다르기 때문에 그 때마다 batch normalization\n",
    "- test set  => 빠르게 속도를 판단해야 하기 때문에 고정된 평균과 표준편차를 사용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Libarary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(55000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(mnist.train.images.shape)\n",
    "print(mnist.train.labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model & Solver Class\n",
    "\n",
    "- 객체지향모델로 모델 구축\n",
    "- model과 solver 을 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "    def __init__(self, name, input_dim, output_dim, hidden_dims = [32,32],\n",
    "                use_batchnorm=True, activation_fn = tf.nn.relu,\n",
    "                oprimizer = tf.train.AdamOptimizer, lr=0.01):\n",
    "        with tf.variable_scope(name): # tf.name_scope 는 그냥 Variable 에만 적용\n",
    "           # tf.variable_scope 은 get_variable 과 Variable 모두에 적용\n",
    "    # tf.variable_scope와 tf.get_variable 의 조합을 통해 텐서를 reuse 할 수도 있음\n",
    "            net = tf.layers.dense"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
