{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.767697 [[1.3909785]\n",
      " [0.6539242]]\n",
      "100 0.7280337 [[0.90191364]\n",
      " [0.50500774]]\n",
      "200 0.7091876 [[0.5875425 ]\n",
      " [0.37484276]]\n",
      "300 0.7004598 [[0.38376015]\n",
      " [0.2699928 ]]\n",
      "400 0.69647104 [[0.25168842]\n",
      " [0.19088158]]\n",
      "500 0.69465727 [[0.1658078 ]\n",
      " [0.13331707]]\n",
      "600 0.6938335 [[0.10969114]\n",
      " [0.0923326 ]]\n",
      "700 0.69345933 [[0.07283541]\n",
      " [0.06356189]]\n",
      "800 0.69328916 [[0.0485146 ]\n",
      " [0.04356049]]\n",
      "900 0.6932118 [[0.03239878]\n",
      " [0.02975221]]\n",
      "[array([[0.49375588],\n",
      "       [0.49884218],\n",
      "       [0.49919787],\n",
      "       [0.50428444]], dtype=float32), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [1.]], dtype=float32), 0.25]\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]],dtype=np.float32)\n",
    "y_data = np.array([[0],[1],[1],[0]], dtype = np.float32)\n",
    "\n",
    "w = tf.Variable(tf.random_normal([2,1])) # y 의 nb_classes 가 2인 binary regression 이기 때문에 one_hot 이 필요하지 않음\n",
    "b = tf.Variable(tf.random_normal([1]))\n",
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(x,w)+b)\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis)+(1-y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "prediction = tf.cast(hypothesis>0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction,y), dtype=tf.float32)) # float 로 해주어야 mean 값도 float 로 나옴\n",
    "# prediction == y 가 아닌 tf.equal(prediction,y) 로 해주어야 각각의 값을 비교 후 bool 값을 반환\n",
    "# == 를 쓰게 되면 전체에 대해서 비교!\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(1000):\n",
    "    sess.run(train, feed_dict={x:x_data,y:y_data})\n",
    "    if step%100==0:\n",
    "        print(step, sess.run(cost, feed_dict={x:x_data,y:y_data}), sess.run(w))\n",
    "        \n",
    "print(sess.run([hypothesis, prediction, accuracy],feed_dict={x:x_data,y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narrow Neural Net for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.705878 [[0.5351454]\n",
      " [1.5684619]]\n",
      "100 0.69290286 [[0.5351454]\n",
      " [1.5684619]]\n",
      "200 0.69284517 [[0.5351454]\n",
      " [1.5684619]]\n",
      "300 0.69278383 [[0.5351454]\n",
      " [1.5684619]]\n",
      "400 0.69271624 [[0.5351454]\n",
      " [1.5684619]]\n",
      "500 0.6926418 [[0.5351454]\n",
      " [1.5684619]]\n",
      "600 0.69255984 [[0.5351454]\n",
      " [1.5684619]]\n",
      "700 0.69246984 [[0.5351454]\n",
      " [1.5684619]]\n",
      "800 0.69237113 [[0.5351454]\n",
      " [1.5684619]]\n",
      "900 0.6922629 [[0.5351454]\n",
      " [1.5684619]]\n",
      "[array([[0.50675327],\n",
      "       [0.50285876],\n",
      "       [0.5003158 ],\n",
      "       [0.4943323 ]], dtype=float32), array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [0.]], dtype=float32), 0.75]\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]],dtype=np.float32)\n",
    "y_data = np.array([[0],[1],[1],[0]], dtype = np.float32)\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,2])) # 2개의 값으로 출력이 되게 설정해줬음, 이건 자기 맘대로 할 수 있음\n",
    "b1 = tf.Variable(tf.random_normal([2])) # w가 2,2 이기 때문에 2!!!\n",
    "layer1 = tf.sigmoid(tf.matmul(x,w1)+b1)\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal([2,1]))\n",
    "b2 = tf.Variable(tf.random_normal([1]))\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1,w2)+b2) # x를 받는 것이 아니라 layer1 을 받는 것임!\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis)+(1-y)*tf.log(1-hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "prediction = tf.cast(hypothesis>0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction,y), dtype=tf.float32)) # float 로 해주어야 mean 값도 float 로 나옴\n",
    "# prediction == y 가 아닌 tf.equal(prediction,y) 로 해주어야 각각의 값을 비교 후 bool 값을 반환\n",
    "# == 를 쓰게 되면 전체에 대해서 비교!\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(1000):\n",
    "    sess.run(train, feed_dict={x:x_data,y:y_data})\n",
    "    if step%100==0:\n",
    "        print(step, sess.run(cost, feed_dict={x:x_data,y:y_data}), sess.run(w))\n",
    "        \n",
    "print(sess.run([hypothesis, prediction, accuracy],feed_dict={x:x_data,y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide NN for XOR  \n",
    "narrow 한 model 보다 더 잘 학습되는 걸 볼 수 있음. hypothesis 의 차이가 커졌고, accuracy 또한 증가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.78784275 [[ 1.0465933]\n",
      " [-1.9999031]]\n",
      "100 0.63813514 [[ 1.0465933]\n",
      " [-1.9999031]]\n",
      "200 0.61180544 [[ 1.0465933]\n",
      " [-1.9999031]]\n",
      "300 0.5836472 [[ 1.0465933]\n",
      " [-1.9999031]]\n",
      "400 0.5526273 [[ 1.0465933]\n",
      " [-1.9999031]]\n",
      "500 0.51844335 [[ 1.0465933]\n",
      " [-1.9999031]]\n",
      "600 0.48120466 [[ 1.0465933]\n",
      " [-1.9999031]]\n",
      "700 0.4414361 [[ 1.0465933]\n",
      " [-1.9999031]]\n",
      "800 0.40014574 [[ 1.0465933]\n",
      " [-1.9999031]]\n",
      "900 0.3586893 [[ 1.0465933]\n",
      " [-1.9999031]]\n",
      "[array([[0.19356015],\n",
      "       [0.68766135],\n",
      "       [0.7700648 ],\n",
      "       [0.34596032]], dtype=float32), array([[0.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [0.]], dtype=float32), 1.0]\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]],dtype=np.float32)\n",
    "y_data = np.array([[0],[1],[1],[0]], dtype = np.float32)\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,10])) # 2개의 값으로 출력이 되게 설정해줬음, 이건 자기 맘대로 할 수 있음\n",
    "b1 = tf.Variable(tf.random_normal([10])) # w가 2,2 이기 때문에 2!!!\n",
    "layer1 = tf.sigmoid(tf.matmul(x,w1)+b1)\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal([10,1]))\n",
    "b2 = tf.Variable(tf.random_normal([1]))\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1,w2)+b2) # x를 받는 것이 아니라 layer1 을 받는 것임!\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis)+(1-y)*tf.log(1-hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "prediction = tf.cast(hypothesis>0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction,y), dtype=tf.float32)) # float 로 해주어야 mean 값도 float 로 나옴\n",
    "# prediction == y 가 아닌 tf.equal(prediction,y) 로 해주어야 각각의 값을 비교 후 bool 값을 반환\n",
    "# == 를 쓰게 되면 전체에 대해서 비교!\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(1000):\n",
    "    sess.run(train, feed_dict={x:x_data,y:y_data})\n",
    "    if step%100==0:\n",
    "        print(step, sess.run(cost, feed_dict={x:x_data,y:y_data}), sess.run(w))\n",
    "        \n",
    "print(sess.run([hypothesis, prediction, accuracy],feed_dict={x:x_data,y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep & Wide NN for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8022101 [[-0.37514785]\n",
      " [ 0.3734707 ]]\n",
      "100 0.6926769 [[-0.37514785]\n",
      " [ 0.3734707 ]]\n",
      "200 0.69224244 [[-0.37514785]\n",
      " [ 0.3734707 ]]\n",
      "300 0.69177866 [[-0.37514785]\n",
      " [ 0.3734707 ]]\n",
      "400 0.6912683 [[-0.37514785]\n",
      " [ 0.3734707 ]]\n",
      "500 0.6906922 [[-0.37514785]\n",
      " [ 0.3734707 ]]\n",
      "600 0.6900291 [[-0.37514785]\n",
      " [ 0.3734707 ]]\n",
      "700 0.6892539 [[-0.37514785]\n",
      " [ 0.3734707 ]]\n",
      "800 0.6883371 [[-0.37514785]\n",
      " [ 0.3734707 ]]\n",
      "900 0.68724215 [[-0.37514785]\n",
      " [ 0.3734707 ]]\n",
      "[array([[0.48941207],\n",
      "       [0.5365239 ],\n",
      "       [0.47214606],\n",
      "       [0.50264114]], dtype=float32), array([[0.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [1.]], dtype=float32), 0.5]\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]],dtype=np.float32)\n",
    "y_data = np.array([[0],[1],[1],[0]], dtype = np.float32)\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,10]))\n",
    "b1 = tf.Variable(tf.random_normal([10]))\n",
    "layer1 = tf.sigmoid(tf.matmul(x,w1)+b1)\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal([10,10]))\n",
    "b2 = tf.Variable(tf.random_normal([10]))\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,w2)+b2)\n",
    "\n",
    "w3 = tf.Variable(tf.random_normal([10,10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, w3)+b3)\n",
    "\n",
    "w4 = tf.Variable(tf.random_normal([10,1]))\n",
    "b4 = tf.Variable(tf.random_normal([1]))\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, w4)+b4)\n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis)+(1-y)*tf.log(1-hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "prediction = tf.cast(hypothesis>0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction,y), dtype=tf.float32)) # float 로 해주어야 mean 값도 float 로 나옴\n",
    "# prediction == y 가 아닌 tf.equal(prediction,y) 로 해주어야 각각의 값을 비교 후 bool 값을 반환\n",
    "# == 를 쓰게 되면 전체에 대해서 비교!\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(1000):\n",
    "    sess.run(train, feed_dict={x:x_data,y:y_data})\n",
    "    if step%100==0:\n",
    "        print(step, sess.run(cost, feed_dict={x:x_data,y:y_data}), sess.run(w))\n",
    "        \n",
    "print(sess.run([hypothesis, prediction, accuracy],feed_dict={x:x_data,y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 어떤 값을 로그할 것인지 정함\n",
    "1. merge로 합치게 됨\n",
    "1. 어느 파일에 summary를 저장할 건지 정하고, 그 파일에 그래프를 저장할 공간도 만들어줌\n",
    "1. 노드 실행, 실제로 파일에 넣어줌\n",
    "1. 텐서보드를 실행시켜줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "w2_hist = tf.summary.histogram(\"weights2\", w2) # 여러개의 값, 즉 하나 들어올 떄 여러개의 값이 존재\n",
    "cost_sum = tf.summary.scalar(\"cost\", cost) # 하나의 값\n",
    "\n",
    "#2\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "#3\n",
    "writer = tf.summary.FileWriter(\"./logs\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "#4\n",
    "\n",
    "s, _ = sess.run([summary, optimizer], feed_dict={x:x_data,y:y_data})\n",
    "writer.add_summary(s, global_step=global_step) # 몇번째인지도 설정\n",
    "\n",
    "#5\n",
    "# Local computer\n",
    "tensorboard --logdir=./logs # 아까의 directory 를 실행시켜주면 됨!\n",
    "#remote server\n",
    "ssh -L local_port:127.0.0.1:remort_port username@server.com\n",
    "        # local_port는 알아서 정해짐, report_port는 정해주어야 함\n",
    "tensorboard --logdir=./logs\n",
    "\n",
    "# http://127.0.0.1:local_port 에 접속함으로써 볼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add scope for better graph hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"layer1\") as scope:\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"layer2\") as scope:\n",
    "    \n",
    "\n",
    "with tf.name_scope(\"layer3\") as scope:\n",
    "    \n",
    "# 를 통해서 각각의 layer을 분리해서 tensorboard를 출력할 수도 있음\n",
    "# Layer 를 클릭하면 그 Layer가 펼쳐짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"./logs/01\")\n",
    "writer = tf.summary.FileWriter(\"./logs/02\")\n",
    "writer = tf.summary.FileWriter(\"./logs/03\")\n",
    "\n",
    "tensorboard --logdir=./logs # 실행시킬 땐 다같이!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "w2_hist = tf.summary.histogram(\"weights2\", w2) # 여러개의 값, 즉 하나 들어올 떄 여러개의 값이 존재\n",
    "cost_sum = tf.summary.scalar(\"cost\", cost) # 하나의 값\n",
    "\n",
    "#2\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "#3\n",
    "writer = tf.summary.FileWriter(\"./logs\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "#4\n",
    "\n",
    "s, _ = sess.run([summary, optimizer], feed_dict={x:x_data,y:y_data})\n",
    "writer.add_summary(s, global_step=global_step) # 몇번째인지도 설정\n",
    "\n",
    "#5\n",
    "# Local computer\n",
    "tensorboard --logdir=./logs # 아까의 directory 를 실행시켜주면 됨!\n",
    "#remote server\n",
    "ssh -L local_port:127.0.0.1:remort_port username@server.com\n",
    "        # local_port는 알아서 정해짐, report_port는 정해주어야 함\n",
    "tensorboard --logdir=./logs\n",
    "\n",
    "# http://127.0.0.1:local_port 에 접속함으로써 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./mobile-train.csv\")\n",
    "train_data = np.array(data, dtype= np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 20)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 20])\n",
    "y = tf.placeholder(tf.int32, [None,1])\n",
    "\n",
    "y_one_hot = tf.one_hot(y,nb_classes)\n",
    "y_one_hot = tf.reshape(y_one_hot, [-1,nb_classes])\n",
    "\n",
    "#layer1,2,3 and hypothesis\n",
    "with tf.name_scope(\"layer1\") as scope:\n",
    "    w1 = tf.Variable(tf.random_normal([20,100]))\n",
    "    b1 = tf.Variable(tf.random_normal([100]))\n",
    "    layer1 = tf.nn.softmax(tf.matmul(x,w1) + b1)\n",
    "    \n",
    "with tf.name_scope(\"layer2\") as scope:\n",
    "    w2 = tf.Variable(tf.random_normal([100,100]))\n",
    "    b2 = tf.Variable(tf.random_normal([100]))\n",
    "    layer2 = tf.nn.softmax(tf.matmul(layer1,w2)+b2)\n",
    "    \n",
    "with tf.name_scope(\"layer3\") as scope:\n",
    "    w3 = tf.Variable(tf.random_normal([100,100]))\n",
    "    b3 = tf.Variable(tf.random_normal([100]))\n",
    "    layer3 = tf.nn.softmax(tf.matmul(layer2,w3)+b3)\n",
    "    \n",
    "with tf.name_scope(\"layer4\") as scope:\n",
    "    w4 = tf.Variable(tf.random_normal([100,nb_classes]))\n",
    "    b4 = tf.Variable(tf.random_normal([nb_classes]))\n",
    "    w4_hist = tf.summary.histogram(\"weight4\", w4)\n",
    "    b4_hist = tf.summary.histogram(\"bias4\",b4)\n",
    "    hypothesis = tf.nn.softmax(tf.matmul(layer3, w4)+b4)\n",
    "\n",
    "# cost and model\n",
    "cost = tf.nn.softmax_cross_entropy_with_logits(logits = tf.matmul(layer3, w4)+b4, labels = y_one_hot)\n",
    "cost_scr = tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "predicted = tf.argmax(hypothesis, axis=1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y_data.flatten()), dtype= tf.float32))\n",
    "\n",
    "summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"./board\")\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "for i in range(1000):\n",
    "    s, a,_ = sess.run([summary, accuracy, train], feed_dict={x:x_data, y:y_data})\n",
    "    writer.add_summary(s, global_step=i)\n",
    "\n",
    "    print(\"정확도 : {:5%}\".format(a))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
