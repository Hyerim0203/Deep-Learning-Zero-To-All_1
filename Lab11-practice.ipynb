{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-aa69bcade290>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, sess, name, learning_rate=0.01):\n",
    "        self.sess=sess\n",
    "        self.name=name\n",
    "        self.learning_rate = learning_rate\n",
    "        self.training = False\n",
    "        self._build_net()\n",
    "    \n",
    "    def _build_net(self):\n",
    "        # placeholder\n",
    "        self.x = tf.placeholder(tf.float32, [None, 784])\n",
    "        x_img = tf.reshape(self.x, [-1,28,28,1])\n",
    "        self.y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "        # Layer1 input -> ?, 28,28,1\n",
    "        l1 = tf.layers.conv2d(inputs = x_img, filters=32, kernel_size = [3,3], strides=[1,1], padding=\"SAME\", \n",
    "    activation=tf.nn.relu, kernel_initializer = tf.contrib.layers.xavier_initializer(), bias_initializer = tf.contrib.layers.xavier_initializer())\n",
    "        l1 = tf.layers.max_pooling2d(inputs=l1, pool_size = [2,2], strides=2, padding=\"SAME\")\n",
    "        l1 = tf.layers.dropout(inputs=l1, rate=0.7, training = self.training)\n",
    "        \"\"\"\n",
    "        w1 = tf.get_variable(\"w1\", shape=[3,3,1,32], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b1 = tf.get_variable(\"b1\", shape=[32], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        l1 = tf.nn.convd2(x_img, w1, strides = [1,1,1,1], padding =\"SAME\") + b1\n",
    "        l1 = tf.nn.relu(l1)\n",
    "        l1 = tf.nn.max_pool(l1, ksize=[1,2,2,1], strides=[1,2,2,1], padding = \"SAME\")\n",
    "        l1 = tf.nn.dropout(l1, keep_prob=keep_prob)\"\"\"\n",
    "        # Layer2 input-> ?,14,14,32\n",
    "        l2 = tf.layers.conv2d(inputs=l1, filters=64, kernel_size=[3,3], strides=[1,1], padding=\"SAME\", activation = tf.nn.relu,\n",
    "    kernel_initializer = tf.contrib.layers.xavier_initializer(), bias_initializer = tf.contrib.layers.xavier_initializer())\n",
    "        l2 = tf.layers.max_pooling2d(inputs=l2, pool_size=[2,2], strides= 2, padding=\"SAME\")\n",
    "        l2 = tf.layers.dropout(inputs=l2, rate=0.7, training=self.training)\n",
    "        \"\"\"\n",
    "        w2 = tf.get_variable(\"w2\", [3,3,32,64], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b2 = tf.get_variable(\"b2\", [64], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        l2 = tf.nn.conv2d(l1, w2, strides = [1,1,1,1], padding=\"SAME\") + b\n",
    "        l2 = tf.nn.relu(l2)\n",
    "        l2 = tf.nn.pool(l2, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "        l2 = tf.nn.dropout(l2, keep_prob = keep_prob)\"\"\"\n",
    "        # Layer3 input -> ?, 7, 7, 64\n",
    "        l3 = tf.layers.conv2d(inputs=l2, filters=128, kernel_size=[3,3],strides=[1,1], padding=\"SAME\", activation = tf.nn.relu,\n",
    "            kernel_initializer = tf.contrib.layers.xavier_initializer(), bias_initializer = tf.contrib.layers.xavier_initializer())\n",
    "        l3 = tf.layers.max_pooling2d(inputs = l3, pool_size = [2,2],strides=2, padding=\"SAME\")\n",
    "        l3 = tf.layers.dropout(inputs=l3, rate=0.7, training=self.training)\n",
    "        l3 = tf.reshape(l3, [-1, 4*4*128])\n",
    "        # Layer4 input -> ?, 4, 4, 128\n",
    "        l4 = tf.layers.dense(inputs=l3, units=600, activation = tf.nn.relu,\n",
    "            kernel_initializer = tf.contrib.layers.xavier_initializer(), bias_initializer = tf.contrib.layers.xavier_initializer())\n",
    "        l4 = tf.layers.dropout(inputs=l4, rate=0.7, training = self.training)\n",
    "        \"\"\"\n",
    "        w4 = tf.get_variable(\"w4\", [4*4*128, 600], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b4 = tf.get_variable(\"b4\", [600], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        l4 = tf.nn.relu(tf.matmul(l3, w4)+b4)\n",
    "        l4 = tf.nn.dropout(l4, keep_prob = keep_prob)\n",
    "        \"\"\"\n",
    "        # Layer5 input -> -1, 600\n",
    "        self.hypothesis = tf.layers.dense(l4, units=10, activation = tf.nn.softmax,\n",
    "     kernel_initializer = tf.contrib.layers.xavier_initializer(), bias_initializer = tf.contrib.layers.xavier_initializer()) \n",
    "        # dropout은 가중치 변수를 줄이기 위해서 하는 건데 더이상 가중치 곱이 없기 때문에 할 필요 X\n",
    "        \"\"\"\n",
    "        w5 = tf.get_variable(\"w5\", [600,10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b5 = tf.get_variable(\"b5\", [10], initializer = tf.contrib.layers.xavier_initializer())\"\"\"\n",
    "        \n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=tf.layers.dense(l4,units=10, kernel_initializer=tf.contrib.layers.xavier_initializer(),bias_initializer=tf.contrib.layers.xavier_initializer()), labels = self.y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "        self.predicted = tf.argmax(self.hypothesis,1)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.predicted, tf.argmax(self.y,1)),dtype=tf.float32))\n",
    "    \n",
    "    def predict(self, x_test, *y_test):\n",
    "        return self.sess.run(self.hypothesis, feed_dict={self.x:x_test})\n",
    "    \n",
    "    def get_accuracy(self,x_test,y_test):\n",
    "        return self.sess.run(self.accuracy, feed_dict={self.x:x_test, self.y:y_test})\n",
    "    def train(self, x_train, y_train):\n",
    "        self.training = True\n",
    "        return self.sess.run([self.cost,self.optimizer], feed_dict={self.x:x_train,self.y:y_train})\n",
    "        \n",
    "   \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "num_emsemble = int(input())\n",
    "models = [Model(sess,\"mmmmmmmmmmmm\"+str(m)) for m in range(num_emsemble)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model train with Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    1 Cost : [2.6071498513221734, 1.54443053483963, 1.2018889643251909, 2.283391987681389, 0.9246945463120939, 1.792877428829669]\n",
      "Epoch :    2 Cost : [2.30303397655487, 0.24557689651846887, 0.19033064190298324, 0.5338413144648074, 0.17390169583261011, 0.276357107758522]\n",
      "Epoch :    3 Cost : [2.302898774147035, 0.17221074588596819, 0.1420222579687834, 0.25009446300566196, 0.11710533281788231, 0.1724373467639089]\n",
      "Epoch :    4 Cost : [2.303658018112183, 0.1329263753816486, 0.10978900396265087, 0.15993022479116917, 0.09810040744021535, 0.14062123492360112]\n",
      "Epoch :    5 Cost : [2.3031508302688586, 0.11517988426610827, 0.1006774523667991, 0.14008260086178786, 0.10487858206033708, 0.12701854706741872]\n",
      "Epoch :    6 Cost : [2.3024141025543208, 0.10257270011119542, 0.08828347455710174, 0.12404754523187878, 0.08906401371583338, 0.1114277506060898]\n",
      "Epoch :    7 Cost : [2.303234069347382, 0.10963509653694924, 0.08710013121366503, 0.10889425227418545, 0.08516605015844109, 0.10129618924111128]\n",
      "Epoch :    8 Cost : [2.302045726776123, 0.0939315736480057, 0.07476777902338652, 0.10293145264498892, 0.08031591919250783, 0.10111858816817401]\n",
      "Epoch :    9 Cost : [2.302105343341828, 0.08611970685422418, 0.06915981023339554, 0.09641212190501393, 0.07465168672148137, 0.10010624565184117]\n",
      "Epoch :   10 Cost : [2.303238897323609, 0.08420522155240177, 0.059716962212696685, 0.08974303794559091, 0.06376945273019373, 0.0812406575633213]\n",
      "Epoch :   11 Cost : [2.3012423563003543, 0.0780865272087976, 0.06253769344650208, 0.0906522040534765, 0.06701766832266001, 0.08788975957781076]\n",
      "Epoch :   12 Cost : [2.3022291922569273, 0.09006567469332366, 0.07818587589543312, 0.09476838385686277, 0.07391958867199719, 0.0908918443135917]\n",
      "Epoch :   13 Cost : [2.302061848640442, 0.07782403124962001, 0.06742588587570937, 0.09000144910067323, 0.07052526914980263, 0.08383171677589417]\n",
      "Epoch :   14 Cost : [2.301789572238922, 0.08975436486303805, 0.07793085973709821, 0.09521085292100903, 0.07568457271438095, 0.09195924568921329]\n",
      "Epoch :   15 Cost : [2.3004962396621695, 0.07051196136977521, 0.04827496893936766, 0.090035215318203, 0.05578014063416049, 0.0758676193957217]\n"
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "num = mnist.train.num_examples//batch_size\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_c = [0 for i in range(len(models))]\n",
    "    for step in range(num):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        for m_idx, m in enumerate(models):\n",
    "            c, _ = m.train(batch_xs,batch_ys)\n",
    "    avg_c[m_idx]+= c/num\n",
    "    print(\"Epoch : %4d\"%(epoch+1), f\"Cost : {avg_c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble prediction using sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "0 model accuracy : 0.0892\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-b996c03bdde5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{m_idx} model accuracy :\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "test_size = len(mnist.test.labels)\n",
    "print(test_size)\n",
    "predictions = np.zeros(test_size*10).reshape(-1,10)\n",
    "\n",
    "for m_idx, m in enumerate(models):\n",
    "    print(f\"{m_idx} model accuracy :\", m.get_accuracy(mnist.test.images, mnist.test.labels))\n",
    "    p=m.hypothesis(feed_dict={m.x:mnist.test.images})\n",
    "    predictions +=p\n",
    "    \n",
    "ensemble_correct_prediction = tf.equal(tf.argmax(predictions,1),tf.argmax(mnist.test.labels,1))\n",
    "ensemble_accuracy = tf.reduce_mean(tf.cast(ensemble_correct_prediction,dtype=tf.float32))\n",
    "\n",
    "print(\"Ensemble accuracy\" , sess.run(ensemble_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep & Wide?\n",
    "# CIFAR 10\n",
    "# imagenet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
