{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model define - cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets.cifar10 import load_data\n",
    "(x_train, y_train),(x_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,sess,name, learning_rate=0.01):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self.learning_rate = learning_rate\n",
    "        self.training=False\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        nb_classes = 10\n",
    "        # placeholder\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.x = tf.placeholder(tf.float32, [None, 32,32,3])\n",
    "            self.y = tf.placeholder(tf.int32, [None,1])\n",
    "            y_one_hot = tf.one_hot(self.y, nb_classes)\n",
    "            y_one_hot = tf.reshape(y_one_hot, [-1, nb_classes])\n",
    "            # layer1  input => -1, 32, 32, 3\n",
    "            l1 = tf.layers.conv2d(inputs=self.x, filters=13, strides=[1,1], kernel_size=[4,4], activation=tf.nn.relu,padding=\"SAME\",\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            l1 = tf.layers.max_pooling2d(inputs=l1, pool_size=[2,2], strides=2, padding=\"SAME\")\n",
    "            l1 = tf.layers.dropout(inputs=l1, rate=0.7, training = self.training)\n",
    "            # layer2  input => -1, 16, 16, 32\n",
    "            l2 = tf.layers.conv2d(inputs=l1, filters = 12, strides=[1,1], kernel_size=[4,4], activation=tf.nn.relu,padding=\"SAME\",\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            l2 = tf.layers.max_pooling2d(l2, pool_size=[2,2], strides=2, padding=\"SAME\")\n",
    "            l2 = tf.layers.dropout(l2, rate=0.7, training = self.training)\n",
    "            # layer3 input => -1, 8, 8, 64\n",
    "            w3 = tf.get_variable(\"w3\", [4,4,12,11], initializer = tf.contrib.layers.xavier_initializer())\n",
    "            b3 = tf.get_variable(\"b3\", [11], initializer = tf.contrib.layers.xavier_initializer())\n",
    "            l3 = tf.nn.conv2d(l2, w3, strides=[1,1,1,1], padding=\"SAME\")+b3\n",
    "            l3 = tf.nn.max_pool(l3, ksize=[1,2,2,1],strides=[1,2,2,1], padding=\"SAME\")\n",
    "            l3 = tf.nn.relu(l3)\n",
    "            l3 = tf.layers.dropout(l3, rate=0.7, training=self.training)\n",
    "    \n",
    "            l3 = tf.reshape(l3, [-1,11*4*4])\n",
    "            # layer4 input =>-1, 4, 4, 128 \n",
    "            l4 = tf.layers.dense(inputs=l3, units = 20, activation = tf.nn.relu, kernel_initializer = tf.contrib.layers.xavier_initializer(), bias_initializer = tf.contrib.layers.xavier_initializer())\n",
    "            l4 = tf.layers.dropout(l4, rate=0.7, training=self.training)\n",
    "            # layer5 input =>\n",
    "            w5 = tf.get_variable(\"w5\", [20 , nb_classes], initializer = tf.contrib.layers.xavier_initializer())\n",
    "            b5 = tf.get_variable(\"b5\", [nb_classes], initializer = tf.contrib.layers.xavier_initializer())\n",
    "            self.logits = tf.matmul(l4, w5) + b5\n",
    "            # model\n",
    "            self.hypothesis = tf.nn.softmax(self.logits)\n",
    "            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels = y_one_hot))\n",
    "            self. optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "            # predicted & accuracy\n",
    "            predicted = tf.argmax(self.hypothesis, 1)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, tf.argmax(y_one_hot,1)), dtype=tf.float32))\n",
    "        \n",
    "    def predict(self, x_test, *y_test):\n",
    "        return self.sess.run(self.hypothesis, feed_dict = {self.x:x_test})\n",
    "    \n",
    "    def get_accuracy(self, x_test, y_test):\n",
    "        return self.sess.run(self.accuracy, feed_dict={self.x:x_test, self.y:y_test})\n",
    "    \n",
    "    def train(self, x_train, y_train):\n",
    "        self.training = True\n",
    "        return self.sess.run([self.cost,self.optimizer], feed_dict = {self.x:x_train, self.y:y_train})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-5-e7f9b63b26ac>:19: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\layers\\convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-e7f9b63b26ac>:20: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-e7f9b63b26ac>:21: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-e7f9b63b26ac>:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-e7f9b63b26ac>:45: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "models = [Model(sess,\"modellllll\"+str(i)) for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : [2.46493936 2.55985636 2.62551793]\n",
      "Epoch 1 : [1.84427803 2.30266379 2.30342132]\n",
      "Epoch 2 : [1.78832807 2.30408348 2.30356842]\n",
      "Epoch 3 : [1.75650998 2.30254214 2.30348953]\n",
      "Epoch 4 : [1.740488   2.26245362 2.30350035]\n",
      "Epoch 5 : [1.71224739 2.00529513 2.30350634]\n",
      "Epoch 6 : [1.71589023 1.90984778 2.30350978]\n",
      "Epoch 7 : [1.70276243 1.85959498 2.30351179]\n",
      "Epoch 8 : [1.69473453 1.85248178 2.30351299]\n",
      "Epoch 9 : [1.67966361 1.8292841  2.30351371]\n",
      "Epoch 10 : [1.68568663 1.81529908 2.30351414]\n",
      "Epoch 11 : [1.72126347 1.81379611 2.30351439]\n",
      "Epoch 12 : [1.70978827 1.7959508  2.30351454]\n",
      "Epoch 13 : [2.02905179 1.827196   2.30351465]\n",
      "Epoch 14 : [1.95862627 1.77303479 2.30351469]\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "num = len(x_train)//batch_size\n",
    "# data\n",
    "#dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "# action\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = np.zeros(len(models))\n",
    "    \n",
    "    for step in range(num):\n",
    "        start_idx = batch_size*step\n",
    "        end_idx = batch_size*(step+1)\n",
    "        batch_x = x_train[start_idx:end_idx,:,:,:]\n",
    "        batch_y = y_train[start_idx:end_idx,:]\n",
    "        for m_idx, m in enumerate(models):\n",
    "            c ,_ = m.train(batch_x, batch_y)\n",
    "            avg_cost[m_idx] += c/num\n",
    "    print(f\"Epoch {epoch} :\", avg_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 0 accuracy : 0.3523\n",
      "model 1 accuracy : 0.3583\n",
      "model 2 accuracy : 0.1\n",
      "ensemble_accuracy : 0.10189999639987946\n"
     ]
    }
   ],
   "source": [
    "ensemble_result = np.zeros(len(x_test)*10).reshape(-1,10)\n",
    "\n",
    "for m_idx, m in enumerate(models):\n",
    "    print(f\"model {m_idx} accuracy :\", m.get_accuracy(x_test,y_test))\n",
    "    p = m.predict(x_test,y_test)\n",
    "    ensemble_result+=p\n",
    "    \n",
    "ensemble_predict = tf.argmax(ensemble_result,1)\n",
    "ensemble_accuracy = tf.reduce_mean(tf.cast(tf.equal(ensemble_predict,tf.argmax(y_test,1)), dtype=tf.float32))\n",
    "print(f\"ensemble_accuracy : {sess.run(ensemble_accuracy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
